---
date: 2025-06-08
---
# Kali GPT - The new age of penetration testing?

[This article](https://cybersecuritynews.com/kali-gpt/) on the "**Kali GPT- AI Assistant That Transforms Penetration Testing on Kali Linux**" has been making the rounds on my feed, and I have spent quite some time pondering it and its implications.

My first thought was along the lines of "how good can it possibly be?". Since it has become the de facto default for many major search engines to lead with an AI generated answer to any question entered, I've witnessed more than just a few blunders when it comes to tool syntax specifically. I've looked up the syntax how to use two password files with hashcat for example, a very basic query, and the AI answer messed up some pretty basic flags, missing a value that let the command fail immediately. The [official documentation](https://hashcat.net/wiki/doku.php?id=combinator_attack) was the way to go (as usual, and as it always should be).
On another occasion, I've set up a test repository on GitHub to play around with, and asked ChatGPT to provide me with some commands to restructure the branches a bit, and the repository was bricked within minutes.

So my faith in these tools to provide syntactically correct and useful commands is limited.
I am however aware of some [impressive results](https://www.hackthebox.com/blog/ai-vs-human-ctf-hack-the-box-results) during CTFs, so maybe a "specialized" GPT could be more attuned to this use case?

However, the more serious implication in my eyes would be if that "seamlessly integrated" tool was actually on a penetration tester's / red teamer's attack machine. Not without reason are NDAs often a big part of the agreements with security vendors. A penetration tester will with some likelihood have access to current and active security weaknesses in a client's infrastructure, employee and customer data (potentially protected bei GDPR, HIPAA, PCI DSS etc.), and an in depth knowledge of how the company's systems are interconnected. (A nice breakdown on NDAs in pre-engagement activities and the pentesting process as a whole can be found in [this Tier 0 Hack The Box module](https://academy.hackthebox.com/course/preview/penetration-testing-process) by the way.)
It seems ludicrous to imagine that this sensitive data should be intertwined in any shape or form with a data hungry GPT and its data sets.

Lastly, there is also the danger of, well, dangerous payloads. The inherent risk for Script Kiddies aside that use code that they do not understand from open-source repositories (and are hence also target of more sophisticated cybercriminals, [poisoning malware creation tools with backdoors](https://news.sophos.com/en-us/2025/06/04/the-strange-tale-of-ischhfd83-when-cybercriminals-eat-their-own/), but more on that in some later post perhaps), a security professional would have to be very diligent in checking whatever payload GPT generates.
The article assures us that the developers emphasize the importance of human oversight due to "*occasionally generate\[d\] false positives or \[...\] unoptimized scripts*". The CIA-triad being upheld during an engagement, not leaving the client with data leaks, loss, or unavailability of mission critical systems, seems much more pressing than an unoptimized script, though.

I am not yet convinced that ChatGPT in any of the current varieties is truly democratizing any industry just yet, as it is so often advertised. It seems more like a shortcut for people that do not want to spend the time and brainpower on researching and understanding a topic. Many technical communities are experiencing the influx of beginners that have followed some AI advice they didn't fully understand, and have subsequently rendered their system damaged beyond repair, looking for a quick fix to get out of it again.

In the foreseeable future, humans will still need to do the critical thinking.

~epsilon